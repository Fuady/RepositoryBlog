<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dplyr on Angga Fuady Blog</title>
    <link>/tags/dplyr/</link>
    <description>Recent content in dplyr on Angga Fuady Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/dplyr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploratory data analysis: Premier league FIFA20</title>
      <link>/blog/2019/11/24/2019-11-24-exploratory-data-analysis-premiere-league-fifa20/</link>
      <pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/11/24/2019-11-24-exploratory-data-analysis-premiere-league-fifa20/</guid>
      <description>This post will explore data of players and clubs in the premier league season 2019/2020 from FIFA20 video games. Data has been scraped from the publicly available website. You may download the available data here which contains the player’s career mode data from FIFA15 to FIFA20. For this post, we focus on the FIFA20 data.
The first thing to do is loading some R packages needed as follows
library(dplyr) library(ggplot2)library(magrittr) library(stringr) library(gridExtra) library(reshape)library(ggcorrplot)Loading dataset: career mode players data in FIFA20fifa &amp;lt;- read.</description>
    </item>
    
    <item>
      <title>Mining and analyzing tweets with R</title>
      <link>/blog/2019/11/17/2019-11-17-mining-and-analyzing-tweets-with-r/</link>
      <pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/11/17/2019-11-17-mining-and-analyzing-tweets-with-r/</guid>
      <description>In this post, I would like to demonstrate simple steps on how to extract and analyze twitter data using R. We mainly use twitteR package to extract information of tweets, and use wordcloud package to visualized word intensity.
The first step is, of course, installing and loading some packages needed as follow
library(twitteR)library(tm)library(ggplot2)library(dplyr)library(wordcloud)library(RColorBrewer)library(stringr)twitteR package is used for extracting tweets’ information. Two packages are utilized for text transformation, namely tm and stringr.</description>
    </item>
    
    <item>
      <title>Scraping IMDB website using rvest </title>
      <link>/blog/2019/11/10/2019-11-10-scraping-imdb-website-using-rvest/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/11/10/2019-11-10-scraping-imdb-website-using-rvest/</guid>
      <description>In this post, we will discuss how to scrap data from a website. As an illustration, we retrieve information on the top 100 best movie all-time form the IMDB website. We mainly use rvest package in R to capture the information from a website.
The results of multiple words are then visualized using ggplot2 and wordcloud, with additional color RcolorBrewer. First thing first, installing and loading the package needed as follows:</description>
    </item>
    
    <item>
      <title>E-commerce data analysis: Customer Behavior</title>
      <link>/blog/2019/11/03/2019-11-03-e-commerce-data-analysis-customer-behaviour/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/11/03/2019-11-03-e-commerce-data-analysis-customer-behaviour/</guid>
      <description>We will perform e-commerce data analysis, more specifically on customer behavior. We are interested in some questions, such as what kind of product that mostly sold? or in which month customers buy more products? or how is the online purchasing trends? , etc.
The data in this post are based on keaggle datasets of e-commerce, you can download here. Four tables of datasets are available, namely, marketing spends, key SKU, online and retail.</description>
    </item>
    
    <item>
      <title>Set date and time using lubridate</title>
      <link>/blog/2019/10/20/2019-10-20-set-date-and-time-using-lubridate/</link>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/10/20/2019-10-20-set-date-and-time-using-lubridate/</guid>
      <description>This post will discuss how to work with date/time data. Here, we will use the R package lubridate and use the NYC flight data nycflights13. There are two types of date/time data that will be covered in this post, namely date (print date only) and date-time (print date and time).
# install packages if needed#install.packages(&amp;quot;lubridate&amp;quot;, &amp;quot;nycflights13&amp;quot;, &amp;quot;ggplot2&amp;quot;)# load librarieslibrary(lubridate)library(nycflights13)library(ggplot2)library(dplyr)Extracting current date or date-timeWe can print the current date or date-time information using today() or now() function.</description>
    </item>
    
    <item>
      <title>Data transformation and manipulation using dplyr</title>
      <link>/blog/2019/10/13/2019-11-23-data-transformation-and-manipulation-using-dplyr/</link>
      <pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/10/13/2019-11-23-data-transformation-and-manipulation-using-dplyr/</guid>
      <description>This time we will discuss on how to transform and manipulate data using the dplyr package. Specifically, here are some topics that will be covered
Filter observations based on their values using filter().Reorder lines using arrange().Selecting variables using select().Create a new variable from an existing variable using mutate().Make a group summary of variables using summarise().Combining several datasets using various join functions.In this post, we use nycflights13 datasets which contains all flights that depaeted from New York City (eg EWR, JFK, and LGA) to destinations in the United States, Puerto Rico and the American Virgin Islands) in 2013 with total of 336,776 flights.</description>
    </item>
    
  </channel>
</rss>