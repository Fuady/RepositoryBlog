---
title: Mining and analyzing tweets with R
banner: img/banners/word.jpg
author: A.M.Fuady
date: '2019-11-17'
slug: mining-and-analyzing-tweets-with-r
categories:
  - R
tags:
  - dplyr
  - wordcloud
  - twitteR
  - ggplot2
  - tm
---



<p>In this post, I would like to demonstrate simple steps on how to extract and analyze twitter data using R. We mainly use <strong>twitteR</strong> package to extract information of tweets, and use <strong>wordcloud</strong> package to visualized word intensity.</p>
<p>The first step is, of course, installing and loading some packages needed as follow</p>
<pre class="r"><code>library(twitteR)
library(tm)
library(ggplot2)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
library(stringr)</code></pre>
<p><strong>twitteR</strong> package is used for extracting tweets’ information. Two packages are utilized for text transformation, namely <strong>tm</strong> and <strong>stringr</strong>. To visualized how often a specific word is chosen, we used <strong>wordcloud</strong>. <strong>dplyr</strong> is used for piping, <strong>RColorBrewer</strong> for multi-color illustration and finally <strong>ggplot2</strong> to plot the bar chart.</p>
<p>After loading all the libraries needed, we need to set up the twitter API connection. This can be done by accessing the developer site of twitter, <a href="https://developer.twitter.com/en/apply-for-access.html">here</a>. In order to get access, you need to have a twitter account and get register to the developer site. Tutorial to access the developer site can be seen <a href="https://medium.com/@GalarnykMichael/accessing-data-from-twitter-api-using-r-part1-b387a1c7d3e">here</a>. After all steps, you will get four codes, namely Consumer key, Consumer Secret, Access Token, and Access Secret. These codes will be used to authorize you for extracting tweets.</p>
<div id="set-up-twitter-api-connection" class="section level2">
<h2>Set up twitter API connection</h2>
<pre class="r"><code># Change the next four lines based on your own consumer_key, consume_secret, access_token, and access_secret. 
consumer_key &lt;- &quot;ClOUrS8CVAp9yGCZQZfjDJIbk&quot;
consumer_secret &lt;- &quot;3S2mru9GgwHrvYIBNV1iTkwgmO5Y9ExfTnOQ91OFlXAu4iDBLw&quot;
access_token &lt;- &quot;1204041700441759744-mDdYIaPPboDPP1HTOenbBiwFP99eWt&quot;
access_secret &lt;- &quot;dtbMBGgDzjjwwTcC7UbwrH7Hd8fNUj6GnqSP9LrWH4WB3&quot;

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)</code></pre>
<pre><code>## [1] &quot;Using direct authentication&quot;</code></pre>
</div>
<div id="searching-tweets-and-visualizing" class="section level2">
<h2>Searching tweets and visualizing</h2>
<p>In this section, I will demonstrate how to extract tweets and visualized them using bar charts and wordcloud. Here, we will search 1000 tweets that contain “data science”.</p>
<pre class="r"><code># Search  some tweets in english containing the words &quot;data science&quot;
ds_tweets = searchTwitter(&quot;data science&quot;, n=1000, lang=&quot;en&quot;)</code></pre>
<p>Now, we have a large list of <code>ds_tweets</code> and then we convert it to a vector of text as follows</p>
<pre class="r"><code># Extract the text from the tweets in a vector
ds_text = sapply(ds_tweets, function(x) x$getText())
head(ds_text)</code></pre>
<pre><code>## [1] &quot;RT @EcoSenseNow: Real data, not wild predictions. Global vulnerability to climate-related hazards has declined 6.5 times for mortality &amp;amp; 5…&quot;
## [2] &quot;RT couponfree01: Udemy Free Discount - Machine Learning A-Z\231: Hands-On Python &amp;amp;amp; R In Data Science #udemycoupon… https://t.co/e9I7TvOANT&quot;
## [3] &quot;RT @EcoSenseNow: Real data, not wild predictions. Global vulnerability to climate-related hazards has declined 6.5 times for mortality &amp;amp; 5…&quot;
## [4] &quot;RT @jgi: Starting 10am PT, Dec. 11 at #AGU19: Learn to use the open science #bioinformatics platform @DOEKBase at the Data Help Desk Demo w…&quot;   
## [5] &quot;RT @DrCristinaF: IMO, this is a sign of science progressing - more interdisciplinary collaborations across the globe, harnessing huge data,…&quot;   
## [6] &quot;RT @EcoSenseNow: Real data, not wild predictions. Global vulnerability to climate-related hazards has declined 6.5 times for mortality &amp;amp; 5…&quot;</code></pre>
<p>Then, we create a corpus which collects all text document for text analysis.</p>
<pre class="r"><code># create a corpus
ds_corpus = Corpus(VectorSource(ds_text))</code></pre>
<p>We perform some transformations of word collection and create a document term matrix as follows</p>
<pre class="r"><code># create document term matrix applying some transformations
tdm = TermDocumentMatrix(ds_corpus,
                         control = list(removePunctuation = TRUE,
                                        stopwords = c(&quot;data&quot;, &quot;science&quot;, stopwords(&quot;english&quot;)),
                                        removeNumbers = TRUE, 
                                        tolower = TRUE))</code></pre>
<p>Here all punctuations and numbers are removed as well as some stop words list and specific words such as “data” and “science”, then all words are suppressed to lower. From the resulting data, the data frame is then constructed.</p>
<pre class="r"><code>#  Obtain words and their frequencies
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE) 
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)</code></pre>
<p>Finally, visualization of the top most frequent word is presented using a bar chart and wordcloud as follows</p>
<pre class="r"><code>ggplot(dm %&gt;% head(10), aes(x= reorder(word, freq) , y = freq, label = freq))+
  geom_bar(stat = &quot;identity&quot;) +
  geom_col(fill = &quot;darkorange&quot;) +
  geom_text( hjust=1.2, color=&quot;white&quot;, size=5) +
  coord_flip()+
  theme_minimal()+
  labs(x = &quot;Word&quot;,
       y = &quot;Frequency&quot;)</code></pre>
<p><img src="/blog/2019-11-17-mining-and-analyzing-tweets-with-r_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># visualized using wordcloud
wordcloud(dm$word, dm$freq , min.freq = 2,
          max.words=200, random.order=FALSE, rot.per=.1,
          colors=brewer.pal(8, &quot;Dark2&quot;))</code></pre>
<p><img src="/blog/2019-11-17-mining-and-analyzing-tweets-with-r_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="hashtag-visualization" class="section level2">
<h2>Hashtag visualization</h2>
<p>Now, we will extract only the hashtag used in the resulting tweets. We use <strong>stringr</strong>, specifically function <em>str_extract_all</em> to extract all words start with <code>#</code>. Then, all hashtags are filed within a data frame.</p>
<pre class="r"><code># hashtag from search results &quot;data science&quot;
# get the hashtags
ds_hashtags = str_extract_all(ds_text, &quot;#\\w+&quot;)
# put tags in vector
df_hashtag &lt;-  ds_hashtags %&gt;% unlist() %&gt;% table() %&gt;% as.data.frame() 
names(df_hashtag) &lt;- c(&quot;hashtag&quot;,&quot;freq&quot;)</code></pre>
<p>Again, we visualized the top hashtag with a bar chart and wordcloud.</p>
<pre class="r"><code>ggplot(df_hashtag %&gt;% arrange(-freq) %&gt;% head(10), aes(x= reorder(hashtag,freq) , y = freq, label = freq))+
  geom_bar(stat = &quot;identity&quot;) +
  geom_col(fill = &quot;darkred&quot;) +
  geom_text( hjust=1.2, color=&quot;white&quot;, size=5) +
  coord_flip()+
  theme_minimal()+
  labs(x = &quot;Hashtag&quot;,
       y = &quot;Frequency&quot;)</code></pre>
<p><img src="/blog/2019-11-17-mining-and-analyzing-tweets-with-r_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code># EPA hashtags wordcloud
wordcloud(df_hashtag$hashtag, df_hashtag$freq , min.freq = 2,
          max.words=200, random.order=FALSE, rot.per=.1,
          colors=brewer.pal(8, &quot;Dark2&quot;))</code></pre>
<p><img src="/blog/2019-11-17-mining-and-analyzing-tweets-with-r_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="account-mentioned-visualization" class="section level2">
<h2>Account mentioned visualization</h2>
<p>Using a similar idea, we extract all accounts mentioned from the resulting search tweets by selecting all words that start by <code>@</code>. Then, we stored the results into a data frame.</p>
<pre class="r"><code># hashtag from search results &quot;data science&quot;
# get the hashtags
ds_mention = str_extract_all(ds_text, &quot;@\\w+&quot;)
# put tags in vector
df_mention &lt;-  ds_mention %&gt;% unlist() %&gt;% table() %&gt;% as.data.frame() 
names(df_mention) &lt;- c(&quot;mention&quot;,&quot;freq&quot;)</code></pre>
<p>Bar chart and wordcloud are used to visualized the top most frequent account mentioned as follows</p>
<pre class="r"><code>ggplot(df_mention %&gt;% arrange(-freq) %&gt;% head(10), aes(x= reorder(mention, freq) , y = freq, label = freq))+
  geom_bar(stat = &quot;identity&quot;) +
  geom_col(fill = &quot;darkgreen&quot;) +
  geom_text( hjust=1.2, color=&quot;white&quot;, size=5) +
  coord_flip()+
  theme_minimal()+
  labs(x = &quot;Mention&quot;,
       y = &quot;Frequency&quot;)</code></pre>
<p><img src="/blog/2019-11-17-mining-and-analyzing-tweets-with-r_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>wordcloud(df_mention$mention, df_mention$freq, min.freq = 2,
          max.words=200, random.order=FALSE, rot.per=.1,
          colors=brewer.pal(8, &quot;Dark2&quot;))</code></pre>
<p><img src="/blog/2019-11-17-mining-and-analyzing-tweets-with-r_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>In this post, I demonstrate how to extract tweets information using R package <strong>twitteR</strong>. Besides that, visualization of most frequent words, hashtag, and account mentioned are also displayed using <strong>wordcloud</strong>.</p>
<p>Thanks for reading this article!</p>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li><a href="https://medium.com/@GalarnykMichael/accessing-data-from-twitter-api-using-r-part1-b387a1c7d3e">Accessing Twitter API</a></li>
<li><a href="https://towardsdatascience.com/a-guide-to-mining-and-analysing-tweets-with-r-2f56818fdd16">Mining and analyzing tweets</a></li>
<li>Photo by Jessicah Hast on Unsplash</li>
</ul>
</div>
</div>
